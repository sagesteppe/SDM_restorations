---
title: "Generate Random Forest Models"
author: "steppe"
date: "2023-10-14"
output: html_document
---

```{r}
library(tidyverse)
library(terra)
library(sf)
source('functions.R')
```

```{r read in predictors}

p <- '/media/steppe/hdd/Geospatial_data/WesternPlantPredictors2'
paths <- list.dirs(p, recursive = T)
paths <- paths[2:length(paths)]
paths <- paths[! grepl('chelsa$|veg_cover$|soilgrids$|sand$|silt$|clay$|phh2o$|cfvo$|soc$', paths) ]

files <- lapply(paths, function(x){ file.path(x, list.files(x)) })
names(files) <- basename(paths)

layers <- lapply(files, vrt)
layers <- rast(layers)
names(layers) <- basename(paths)

rm(paths, files)
```

```{r read in response data and model, eval = F}
p <- '../data/raw/occurrence/combined_records'
p2f <- file.path(p, list.files(p, pattern = '.shp$'))
train_dat <- lapply(p2f, sf::st_read, quiet = TRUE)
train_dat <- bind_rows(train_dat) %>% 
  st_transform(crs(layers))

splicies <- split(train_dat, f = train_dat$taxon)

cores <- parallel::detectCores()
lapply(splicies, modeller) 

```

Model the missing AZ strip species
```{r re run on missing az strip species, eval = F}

targets <- read.csv('../data/raw/2024sos_species.csv', na.strings = "") %>% 
  
  ## we will not model a few species which are widely abundant, hence do not require modelling, and would utilize enormous computational resources.
  filter(!species %in% c('tridentata', 'millefolium', 'dioica', 'negundo')) %>%  # purshia,
  unite('taxon', genus:infraspecies, na.rm = T, sep = ' ') %>% # artemisia, and larrea !
  mutate(taxon = str_squish(taxon))

target_taxa1 <- c('Brodiaea elegans', 'Krameria bicolor', 'Lotus humistratus', 'Adenostoma fasciculatum')

target_taxa <- read.csv('../data/raw/2024sos_species-FIXED.csv', na.strings = "") %>% 
  filter(!species %in% c('tridentata', 'millefolium', 'dioica', 'negundo')) %>%  # purshia,
  unite('taxon', genus:infraspecies, na.rm = T, sep = ' ') %>% # artemisia, and larrea !
  mutate(taxon = str_squish(taxon)) %>% 
  filter(! taxon %in% c(targets$taxon, target_taxa1)) %>% 
  pull(taxon)

p <- '../data/raw/occurrence/combined_records'
p2f <- file.path(p, list.files(p, pattern = '.shp$'))
p2f <- p2f[grep(paste(paste0(gsub(' ', '_', target_taxa), '.shp'), collapse = '|'), basename(p2f))]

train_dat <- lapply(p2f, sf::st_read, quiet = TRUE)
train_dat <- bind_rows(train_dat) %>% 
  st_transform(crs(layers))

splicies <- split(train_dat, f = train_dat$taxon)

cores <- parallel::detectCores()
lapply(splicies, modeller) 
```





```{r summarize results from RF models}

psum <- '../results/summary'
f <- file.path(psum, list.files(psum))

summary <- lapply(f, read.csv)
names(summary) <- gsub('.csv', '',  basename(f))
summary <- data.table::rbindlist(summary, idcol = 'species')

filter(summary, Metric == 'Balanced Accuracy') %>% 
  ggplot(aes(x = Value)) + 
  geom_histogram() +
  geom_density() 

filter(summary, Metric == 'Sensitivity') %>% 
  ggplot(aes(x = Value)) + 
  geom_histogram()
```

